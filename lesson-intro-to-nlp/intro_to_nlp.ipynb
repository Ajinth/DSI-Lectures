{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, all imports for this notebook are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, \\\n",
    "HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_union, make_pipeline, Pipeline\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Natural Language Processing \n",
    "\n",
    "Natural language processing is a fairly large field and one that we will not cover in nearly enough detail this week. For our purposes, it involves the processing of written language into numbers that we will then use during modeling. This puts it squarely in the preprocessing / feature engineering stage.\n",
    "\n",
    "> Note, there are plenty of cases where we may want to predict what the next word of a statement will be or to automatically provide an answer to a question, program a chat bot, etc. These also fall under the NLP umbrella, but for this week's discussions, we'll focus on just going from language -> numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example\n",
    "\n",
    "Suppose we are building a spam/ham classifier. Input are emails, output is a binary classification of whether or not the email is a spam message or not.\n",
    "\n",
    "Here's an example of an input email:\n",
    "\n",
    "> Hello, I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
    "\n",
    "and a second example:\n",
    "\n",
    "> Hello, I am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n",
    "\n",
    "We might want to look at certain words and determine where they happen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 1 (10 Minutes)\n",
    "\n",
    "With a partner, do the following:\n",
    "\n",
    "1. Identify which email is a spam email and which is a ham email. \n",
    "2. What words exist in the spam email that do not exist in the ham email? What about the reverse?\n",
    "3. Using these words, create a rule that would classify a spam email and do not share it with your partner.\n",
    "> For example, the rule could be \"If the email refers to cancer, it is a spam email\"\n",
    "4. Go online (or into your inbox!) and find a second example of a spam email. PM this text to your partner. Does their rule correctly classify this email?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words model is a simplifying representation used in natural language processing. In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. In other words, how many times does each word happen?\n",
    "\n",
    "One way to do this easily is with the `Counter` module in the standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = '''\n",
    "Hello, I saw your contact information on LinkedIn. \n",
    "I have carefully read through your profile and you seem to have an outstanding personality. \n",
    "This is one major reason why I am in contact with you.\n",
    "My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". \n",
    "I am 86 years old and I was diagnosed with cancer 2 years ago. \n",
    "I will be going in for an operation later this week. \n",
    "I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
    "'''\n",
    "\n",
    "ham = '''\n",
    "Hello, I am writing in regards to your application to the position of Data Scientist at Hooli X.\n",
    "We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site \n",
    "interview with our Senior Data Scientist Mr. John Smith. \n",
    "You will find attached to this message further information on date, time and location of the interview.\n",
    "Please let me know if I can be of any further assistance. Best Regards.\n",
    "'''\n",
    "\n",
    "print(Counter(spam.lower().split()))\n",
    "print('\\n')\n",
    "print(Counter(ham.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(spam.filter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, outside of prepositions and articles, most words are unique across these documents. However, for the ham email, the following interesting words happen more than once:\n",
    "\n",
    "- data\n",
    "- scientist\n",
    "- further\n",
    "\n",
    "and in the spam email:\n",
    "\n",
    "- contact\n",
    "- years\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn Options\n",
    "\n",
    "Scikit-learn offers two different options for Bag-of-Words approaches to NLP:\n",
    "\n",
    "- [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "- [`HashingVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)\n",
    "\n",
    "In **most** cases, you'll be using the `CountVectorizer` object, but there may be times, especially if you are on an older machine, that `HashingVectorizer` will be a huge timesaver.\n",
    "\n",
    "> **Major Note**: NLP can take a lot of computing power. Don't be afraid to look for alternatives like `HashingVectorizer`, to cut your dataset down, to use fewer words, use a larger virtualized machine (such as on AWS). Working smart, not hard applies very well here!\n",
    "\n",
    "[`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) will take a set of words and split them up into one column per word, with (by default) the count of the word for that row in that column. We'll walk through a brief example now and then dive into the options. \n",
    "\n",
    "This library is very full-featured and has a number of options to set or tweak -- get used to reviewing those options to make sure that you are gaining the most out of your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame().from_dict({0: spam, 1: ham}, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(df['text'])\n",
    "cv.transform(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is sparse matrix? Transformations like this tend to create matrices with a very large amount of zeroes (words may only show up once or twice across a number of documents, so a lot of extra memory is expended to represent a very large number of zeroes).\n",
    "\n",
    "Sparse matrices collapse regular (dense) matrices by marking down only cases where a non-zero value is found for a certain combination of row and column. It then drops all the zeroes, allowing for a reduced memory footprint.\n",
    "\n",
    "We can call `.todense()` on the output to convert it into a dense matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.transform(df['text']).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can grab the feature names and apply them if we are interested in seeing the distribution of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(cv.transform(df['text']).todense(),\n",
    "                    columns=cv.get_feature_names()) # use .get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the random numbers at the front? That comes from the `8,750,000.00` and `86` sections of the spam email -- our parser does not interpret `8,750,000.00` as one number but rather the string of words `[8, 750, 000, 00]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CountVectorizer` Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` takes the following (useful) keyword arguments:\n",
    "\n",
    "| Argument | Default Value | Definition |\n",
    "| :--- | :--- | :--- |\n",
    "| `decode_error` | `strict` | What to do if text cannot be decoded. `strict` will raise a `UnicodeDecodeError`, `ignore` will skip that word, `replace` will attempt to replace it with a non-Unicode variant|\n",
    "| `strip_accents` | `None` | When preprocessing a word, `CountVectorizer` does nothing with the accented characters. `ascii` will convert those characters if they have a direct ASCII mapping (à -> a, for example), and `unicode` is slower but will do it for all characters | \n",
    "| `preprocessor / tokenizer` | `None` | Ways to override how to split text into words (`tokenizer`) and what to do with those words before vectorizing (`preprocessor`) -- we'll discuss shortly |\n",
    "| `ngram_range` | `(1, 1)` | Sometimes we may want each sequence of _n_ words as well as each individual word. This is known as an _ngram_ and we cna set that here | \n",
    "| `stop_words` | `None` | Whether or not to remove stop words. Will discuss later. |\n",
    "| `max_df` | `1.0` | `df` refers to the document frequency -- how often does a given word show up across documents. If we set this to a float less than 1.0, any word that happens more frequently than that value will be discarded. Any integer will be the number of documents instead of the proportion | \n",
    "| `min_df` | `1` | Same as `max_df`, but for the number / proportion of documents that a word has to appear in before it is included |\n",
    "| `max_features` | `None` | The top _n_ occuring features to include. If `None`, include all features. This is a great way to coerce `CountVectorizer` to return a matrix of a specific shape / size |\n",
    "| `binary` | `False` | If `True`, return dummy variables instead of a count of occurances |\n",
    "\n",
    "Whew! That's a lot!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 2 (15 Minutes)\n",
    "\n",
    "For this Check for Understanding (and many others), we'll be using the [`20 Newsgroups`](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html) dataset. This dataset includes a number of old messages from a set of newsgroups and is a standard dataset to practice NLP techniques with. \n",
    "\n",
    "For this first section, we will be using the first 100 messages from the `sci.space` newsgroup. We'll also be stripping them of any headers, footers, or quoted messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self practice review (12/08/2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = fetch_20newsgroups(subset='train', categories=['sci.space'],\n",
    "                          remove=['headers', 'footers', 'quotes'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_messages = space['data'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', min_df=0.01)\n",
    "space_words = cv.fit_transform(space_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(space_words.todense(), \n",
    "                     columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words.head()\n",
    "print(words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', \n",
    "                     min_df=0.01, ngram_range=(1,2))\n",
    "space_words = pd.DataFrame(cv.fit_transform(space_messages).todense(),\n",
    "                          columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_words.sum().sort_values(ascending=False).head()\n",
    "print(space_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english',min_df=0.1, max_df=0.9)\n",
    "space_words = pd.DataFrame(cv.fit_transform(space_messages).todense(),\n",
    "                          columns=cv.get_feature_names())\n",
    "print(space_words.sum().sort_values(ascending=False).head())\n",
    "print(space_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,95))\n",
    "sns.heatmap(space_words.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=100)\n",
    "space_words = pd.DataFrame(cv.fit_transform(space_messages).todense(),\n",
    "                          columns=cv.get_feature_names())\n",
    "space_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(space_words.shape)\n",
    "print(space_words.sum().sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = fetch_20newsgroups(subset='train',\n",
    "                          categories=['sci.space'],\n",
    "                          remove=('headers', 'footers', 'quotes'))\n",
    "space_messages = space['data'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(space_messages[0], '\\n', space_messages[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(space_messages)\n",
    "sm = cv.transform(space_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sm.todense(),columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "cv.fit(space_messages)\n",
    "smn = cv.transform(space_messages)\n",
    "df_smn = pd.DataFrame(smn.todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smn.sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=0.10, max_df=.90)\n",
    "smm = cv.fit_transform(space_messages)\n",
    "df_smm = pd.DataFrame(smm.todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smm.sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv= CountVectorizer(max_features=100)\n",
    "smm2 = cv.fit_transform(space_messages)\n",
    "df_smm2 = pd.DataFrame(smm2.todense(), columns=cv.get_feature_names())\n",
    "df_smm2.sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df_smn.shape)\n",
    "print(df_smm.shape)\n",
    "print(df_smm2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.heatmap(df_smm2.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`space_messages` now contains 100 space-themed messages that we will do some feature extraction on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individually, please try the following:\n",
    "\n",
    "1. Use `CountVectorizer` (at its default settings) and Pandas to create a DataFrame where each column is a word in the text. \n",
    "    1. How large is your DataFrame? \n",
    "    2. What are the top 10 occuring words?\n",
    "2. Reinstantiate `CountVectorizer` and set `ngram_range` to `(1, 2)`, then fit and transform the space messages into a DataFrame\n",
    "    1. What does this do?\n",
    "    2. How large is your DataFrame?\n",
    "    3. What are the top 10 occuring words?\n",
    "3. Reinstatiate `CountVectorizer` and set `min_df` to `0.10` and `max_df` to `0.90`, then fit and transform the space messages into a DataFrame\n",
    "    1. What does this do?\n",
    "    2. How large is your DataFrame?\n",
    "    3. What are the top 10 occuring words?\n",
    "4. Reinstate `CountVectorizer` and set `max_features` to `100`, then fit and transform the space messages into a DataFrame\n",
    "    1. What does this do?\n",
    "    2. How large is your DataFrame?\n",
    "    3. What are the top 10 occuring words?\n",
    "5. Pick your smallest DataFrame and do a correlation heatmap on it. Do you see any trends? Do words co-occur together?\n",
    "\n",
    "When finished, with a partner or a small group, answer the following:\n",
    "\n",
    "6. What are each of these techniques doing? Which words are being dropped with each different technique? Which technique do you think captures the most useful words from the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain words occur a huge amount but may not offer anything useful to us as modelers. We'll discuss removing these words shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `HashingVectorizer`\n",
    "\n",
    "Sometimes we may have too many features or too many rows and run out of memory trying to process all of it. Sklearn provides a second option known as `HashingVectorizer` to avoid that.\n",
    "\n",
    "- Benefit: Low memory footprint, allowing you to do more work\n",
    "- Downside: No ability to determine which feature corresponds to what the original word is.\n",
    "\n",
    "In most cases, we'll be using `CountVectorizer`, but `HashingVectorizer` can be a nice ace in the hole. \n",
    "\n",
    "#### How it Works\n",
    "\n",
    "As you have seen we can set the `CountVectorizer` dictionary to have a fixed size, only keeping words of certain frequencies, however, we still have to compute a dictionary and hold the dictionary in memory. This could be a problem when we have a large corpus or in streaming applications where we don't know which words we will encounter in the future.\n",
    "\n",
    "These problems can be solved using the `HashingVectorizer`, which converts a collection of text documents to a matrix of occurrences, calculated with the hashing trick. Each word is mapped to a feature with the use of a hash function that converts it to a hash. If we encounter that word again in the text, it will be converted to the same hash, allowing us to count word occurence without retaining a dictionary in memory. This is very convenient!\n",
    "\n",
    "The main drawback of the this trick is that it's not possible to compute the inverse transform, and thus we lose information on what words the important features correspond to. The hash function employed is the signed 32-bit version of Murmurhash3.\n",
    "\n",
    "We'll instantiate `HashingVectorizer` with the keyword argument `norm=None` to get back real value counts of words. The default is to normalize these values so as to prevent hashing collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvec = HashingVectorizer(stop_words='english', norm=None)\n",
    "hvec.fit(space_messages)\n",
    "hvec.n_features\n",
    "\n",
    "answer = pd.DataFrame(hvec.transform(space_messages).todense())\n",
    "answer.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvec.get_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, slightly different version of a word exist. For example: LinkedIn sees 6000+ variations of the title \"Software Engineer\" and 8000+ variations of the word \"IBM\".\n",
    "\n",
    "It would be wrong to consider the words \"MR.\" and \"mr\" to be different features, thus we need a technique to normalize words to a common root. This technique is called Stemming.\n",
    "\n",
    "- Science, Scientist => Scien\n",
    "- Swimming, Swimmer, Swim => Swim\n",
    "\n",
    "We could define a Stemmer based on rules that we've decided on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokens):\n",
    "    '''rules-based stemming of a bunch of tokens'''\n",
    "    \n",
    "    new_bag = []\n",
    "    for token in tokens:\n",
    "        # define rules here\n",
    "        if token.endswith('s'):\n",
    "            new_bag.append(token[:-1])\n",
    "        elif token.endswith('er'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('tion'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('tist'):\n",
    "            new_bag.append(token[:-4])\n",
    "        elif token.endswith('ce'):\n",
    "            new_bag.append(token[:-2])\n",
    "        elif token.endswith('ing'):\n",
    "            new_bag.append(token[:-2])\n",
    "        else:\n",
    "            new_bag.append(token)\n",
    "\n",
    "    return new_bag\n",
    "\n",
    "stem(['Science', 'Scientist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it is often easier to make use of `nltk` to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('general'))\n",
    "print(stemmer.stem('scientists'))\n",
    "print(stemmer.stem('scientist'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some words are very common and provide no information on the text content.\n",
    "\n",
    "We should remove these stop words. Every language has different stop words and you can add your own words as well if it makes sense (for example, if you there was a domain word that was prevelant but ultimately uninformative, you should use your judgement to remove it).\n",
    "\n",
    "`nltk` provides their own list of stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop_russian = stopwords.words('russian')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `CountVectorizer` to remove common stopwords for us by setting the `stop_words` keyword argument to `english`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(space_messages)\n",
    "stopped_words = pd.DataFrame(cv.transform(space_messages).todense(),\n",
    "                              columns=cv.get_feature_names())\n",
    "print(stopped_words.sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(space_messages)\n",
    "stopped_words = pd.DataFrame(cv.transform(space_messages).todense(),\n",
    "                              columns=cv.get_feature_names())\n",
    "print(stopped_words.sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.extend(['grace', 'jense'])\n",
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dramatically changes the type of \"common\" words available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'i am a data scientist'\n",
    "\n",
    "for word in sent.split():\n",
    "    if word not in stop:\n",
    "        print(word)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to combine transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with text data requires a lot of preprocessing, as the effectiveness of your model can dramatically change based on how well you've processed your text data. \n",
    "\n",
    "In **most** cases, tweaking the settings of `CountVectorizer` will generally be ok. However, sometimes you may want to create a custom function to preprocess text for `CountVectorizer`.\n",
    "\n",
    "Imagine that I wanted to do the following to my text:\n",
    "\n",
    "- Remove punctuation and numbers\n",
    "- Turn text lowercase\n",
    "- Remove stop words\n",
    "- Stem remaining words\n",
    "\n",
    "We could imagine building a function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop = stopwords.words('english')\n",
    "    text = text.translate(str.maketrans('','', string.punctuation))\n",
    "    text = text.translate(str.maketrans('','', string.digits))\n",
    "    text = text.lower.strip()\n",
    "    final_text = []\n",
    "    for w in text.split():\n",
    "        if w not in stop:\n",
    "            final_text.append(stemmer.stem(w.strip()))\n",
    "    return ' '.join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop = stopwords.words('english')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    text = text.lower().strip()\n",
    "    final_text = []\n",
    "    for w in text.split():\n",
    "        if w not in stop:\n",
    "            final_text.append(stemmer.stem(w.strip()))\n",
    "    return ' '.join(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That would change our original spam message from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_cleaned = cleaner(spam)\n",
    "print(spam_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass this function into `CountVectorizer` as a way to preprocess the text as a part of the fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(preprocessor=cleaner)\n",
    "custom_preprocess = pd.DataFrame(cv.fit_transform(space_messages).todense(),\n",
    "                                columns=cv.get_feature_names())\n",
    "print(custom_preprocess.sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_preprocess.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(preprocessor=cleaner)\n",
    "cv.fit(space_messages)\n",
    "custom_preprocess = pd.DataFrame(cv.transform(space_messages).todense(),\n",
    "                              columns=cv.get_feature_names())\n",
    "print(custom_preprocess.sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use a custom preprocessor callable\n",
    "\n",
    "Starting off, I'd make use of what's built into `CountVectorizer` and see if that changes your outcome. However, if you've been working on a dataset for a longer amount of time (such as during your projects or your Capstone) you may want to start developing custom preprocessors that make more sense for the domain that you are in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency - Inverse document Frequency (tf-idf)\n",
    "\n",
    "Tf-idf tells us which words are the most discriminating between documents. Words that occur a lot in one document but don't occur in many other documents tells you a lot more than a word that occurs frequently in all documents.\n",
    "\n",
    "Take a look at the map below. These types of maps usually highlight the most **uniquely favorite food** of a state, not the actual favorite food (which in most cases is probably pizza).\n",
    "\n",
    "Tf-idf, conceptually, does the same thing: it highlights what is common or typical in one or two cases and rare in all others. That's usually more interesting (and predictive) then words or items that are common everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/map-foods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Term frequency is the frequency of a certain term in a document\n",
    "- Inverse document frequency is defined as the frequency of documents that contain that term over the whole corpus.\n",
    "\n",
    "This technique reweights terms to strengthen those that are highly specific to a particular document, while suppressing terms that are common to most documents.\n",
    "\n",
    "Formally, you'll usually see it like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ tf(t, d) = \\frac{N_{term}}{N_{document}} $$\n",
    "\n",
    "$$ idf(t, D) = log(\\frac{N_{documents}}{N_{\\text{documents that contain term t}}}) $$\n",
    "\n",
    "We'll use sklearn's `TfidfVectorizer` to count vectorize and then transform that data using Tfidf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "tvec.fit([spam, ham])\n",
    "\n",
    "df = pd.DataFrame(tvec.transform([spam, ham]).todense(),\n",
    "                   columns=tvec.get_feature_names(),\n",
    "                   index=['spam', 'ham'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see that:\n",
    "\n",
    "- words that don't show up a document have `0.0000` for values\n",
    "- words that do show up in a document are weighted based on tf-idf -- as those scores increase in value, the those terms occur more uniquely in document _d_ than in other documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 3 (15 minutes)\n",
    "\n",
    "Let's practice some of these new techniques on `space_messages` that we established previously. As a reminder, these contain the first 100 messages in the space newsgroups dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(space_messages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individually, please try and tackle the following questions. You may find it helpful to use some of the code you wrote for the first Check for Understanding.\n",
    "\n",
    "1. Use the default `CountVectorizer()` to create a DataFrame and answer the following questions (note, we also did this same prompt in the first CfU -- this is to remind us of where we're starting from):\n",
    "    - How large is your DataFrame?\n",
    "    - What are the top 10 occuring words?\n",
    "2. Use `CountVectorizer()` and remove stop words with the `stop_words` keyword argument. \n",
    "    - How large is your DataFrame?\n",
    "    - What are the top 10 occuring words?\n",
    "3. Use `TfIdfVectorizer()` and remove stop words with the `stop_words` keyword argument.\n",
    "    - How large is your DataFrame?\n",
    "    - Interpret the values that are in the DataFrame. What do they mean?\n",
    "4. We've tried a bunch of different ways to transform the same text data into numbers across this check for understanding and the previous one. Pick two methods and compare and contrast them. What type of words are included in each DataFrame? What types of words are removed or reduced in impact?\n",
    "\n",
    "When you're finished working individually, share your work with a partner. Did you come to the same conclusions? What happens if you try this with new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "cv_sm = cv.fit_transform(space_messages)\n",
    "df_new = pd.DataFrame(cv_sm.todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(stop_words='english')\n",
    "tvsd = pd.DataFrame(tv.fit_transform(space_messages).todense(),\n",
    "                   columns=cv.get_feature_names())\n",
    "tvsd.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to apply NLP\n",
    "\n",
    "Typically, we'll use NLP in the feature engineering stage to help us create new features for our data, which we'll then use in either structured or unstructured modeling techniques. What follows is an example of how we might apply this process. We'll first do this in an ad-hoc fashion, then refactor our code for a more production-ready format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a truncated dataset about the news. This is a set of 8000 randomly selected news articles with an indicator as to whether the news article is discussing economic news or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>The Morning Brief: White House Seeks to Limit ...</td>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>Banking Bill Negotiators Set Compromise --- Pl...</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>Manager's Journal: Sniffing Out Drug Abusers I...</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relevance                                           headline  \\\n",
       "0       yes              Yields on CDs Fell in the Latest Week   \n",
       "1        no  The Morning Brief: White House Seeks to Limit ...   \n",
       "2        no  Banking Bill Negotiators Set Compromise --- Pl...   \n",
       "3        no  Manager's Journal: Sniffing Out Drug Abusers I...   \n",
       "4       yes  Currency Trading: Dollar Remains in Tight Rang...   \n",
       "\n",
       "                                                text  \n",
       "0  NEW YORK -- Yields on most certificates of dep...  \n",
       "1  The Wall Street Journal Online</br></br>The Mo...  \n",
       "2  WASHINGTON -- In an effort to achieve banking ...  \n",
       "3  The statistics on the enormous costs of employ...  \n",
       "4  NEW YORK -- Indecision marked the dollar's ton...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "econ = pd.read_csv('datasets/economic_news.csv', \n",
    "                   usecols=[7, 11, 14])\n",
    "econ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bummer of a Recovery; On economic growth, real GDP has risen 0.8% over the 13 quarters since the recession began, compared to an average increase of 9.9% in past recoveries.\n"
     ]
    }
   ],
   "source": [
    "print(econ.iloc[61,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dummify the `relevance` column and strip out `</br>` from our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ['relevance'] = econ['relevance'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "econ['text'] = econ['text'].apply(lambda x: x.replace('</br>', ''))\n",
    "econ['headline'] = econ['headline'].apply(lambda x: x.replace('</br>',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ['relevance'] = econ['relevance'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "econ['text'] = econ['text'].apply(lambda x: x.replace('</br>', ''))\n",
    "econ['headline'] = econ['headline'].apply(lambda x: x.replace('</br>', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(3):\n",
    "    print(econ.iloc[0,x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's split into a training set and a test set, using the `text` of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(econ['text'].values, \n",
    "                                                   econ['relevance'].values,\n",
    "                                                   test_size=0.33,\n",
    "                                                   random_state=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming data using `CountVectorizer`\n",
    "\n",
    "We'll use `CountVectorizer` in its default form to prepare a dataframe for modeling. Note that I'm going to leave this in its sparse form, because it typically won't matter for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', min_df=0.01)\n",
    "cv.fit(X_train)\n",
    "X = cv.transform(X_train)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction with PCA\n",
    "\n",
    "Typically, those 38,000+ columns are not individually informative. Reducing them in dimensionality using PCA can be very helpful. We'll be using a variant of PCA known as [`TruncatedSVD`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) -- it does more or less the same thing as PCA but in a slightly different fashion. It's best used with sparse matrices like these. \n",
    "\n",
    "**Note**: `TruncatedSVD` works on the actual matrix itself, _not_ the covariance matrix like PCA. This means that the maximum number of components you can make is limited to the smaller of your rows or columns. In this case, even though we have a large number of columns, we only have around 5,000 rows, meaning that the largest number of components `TruncatedSVD` will be able to make is that number.\n",
    "\n",
    "First, we'll pick a high number of components and graph the explained variance ratio to see if there's a useful cutoff point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=200)\n",
    "tsvd.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(200),tsvd.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=200)\n",
    "tsvd.fit(X)\n",
    "plt.plot(range(1000), tsvd.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph would suggest that somewhere north of 100 components might be pretty good (it is where the rate of change begins to flatten out). I'm going to stick with a smaller number (10) for the time being for modeling speed. This will be one of the things we tweak in our Check for Understanding later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=200)\n",
    "tsvd.fit(X)\n",
    "X_tsvd = tsvd.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting with `RandomForestClassifier`\n",
    "\n",
    "At this point, we can move forward with modeling on our new sparse matrix of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_tsvd, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfc.score(X_tsvd, y_train))\n",
    "print(confusion_matrix(y_train, rfc.predict(X_tsvd)))\n",
    "print(classification_report(y_train, rfc.predict(X_tsvd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying test data with RandomForestClassifier\n",
    "X_test_cv = cv.transform(X_test)\n",
    "X_test_svd = tsvd.transform(X_test_cv)\n",
    "print(rfc.score(X_test_svd, y_test))\n",
    "\n",
    "print(classification_report(y_test, rfc.predict(X_test_svd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = pd.DataFrame(confusion_matrix(y_test, rfc.predict(X_test_svd)),\n",
    "                  columns=['Predicted 0', 'Predicted 1'], \n",
    "                  index=['Actual 0', 'Actual 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_tsvd, y_train)\n",
    "print(rfc.score(X_tsvd, y_train))\n",
    "print(confusion_matrix(y_train, rfc.predict(X_tsvd)))\n",
    "print(classification_report(y_train, rfc.predict(X_tsvd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train, rfc.predict(X_tsvd))\n",
    "pd.DataFrame(cm, columns=['Predict_0', 'Predict_1'],index=['Actual_0', 'Actual_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(y_train, rfc.predict(X_tsvd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an exceptionally good model with high accuracy, though we do misclassify about 12% of our class 1 (economic) news as non-economic news. However, we should check to make sure that it works equally well on our test data. As before, we'll make sure that we are applying the already fit transformation to our test data, not refitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cv = cv.transform(X_test)\n",
    "X_test_svd = tsvd.transform(X_test_cv)\n",
    "print(rfc.score(X_test_svd, y_test))\n",
    "print(confusion_matrix(y_test, rfc.predict(X_test_svd)))\n",
    "print(classification_report(y_test, rfc.predict(X_test_svd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a little worse here, as expected -- we almost completely misclassify all of our economic news as non-economic news. However, this process does identify our general process for modeling with text data:\n",
    "\n",
    "1. Use the NLP techniques that we've identified to clean and prepare the data\n",
    "2. Consider using a dimensionality reduction technique like `TruncatedSVD`\n",
    "3. Use the prepared data with machine learning techniques that we've already seen before to predict things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 3 (20 minutes)\n",
    "\n",
    "This is an open ended check for understanding. Your goal at the end of 20 minutes is to have a better model with this same data that maximizes the class 1 recall of our model on the test data:\n",
    "\n",
    "> Our current model at this point correctly predicts 24 of 486 economic articles correctly -- we want to increase the predictive power of our model for that test case!\n",
    "\n",
    "How you do so is up to you (exploring how the data is transformed is highly encouraged). Some options:\n",
    "\n",
    "- Use stop words and other arguments in `CountVectorizer` to change which features are engineered\n",
    "- Make use of `TfidfVectorizer` to highlight more unique words\n",
    "- Modify your the number of components created in `TruncatedSVD`\n",
    "- Try a different modeling technique\n",
    "- Use `GridSearchCV` to optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(econ['text'].values, \n",
    "                                                   econ['relevance'].values,\n",
    "                                                   test_size=0.33,\n",
    "                                                   random_state=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5360,)\n",
      "(5360,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5360x1928 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 370173 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english', min_df=0.01)\n",
    "X_cv = cv.fit_transform(X_train)\n",
    "X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(stop_words='english', min_df=0.01)\n",
    "X_tv = tv.fit_transform(X_train)\n",
    "X_tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd_cv = TruncatedSVD(n_components=1000)\n",
    "X_cv_tsvd = tsvd_cv.fit_transform(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd_xtv = TruncatedSVD(n_components=1000)\n",
    "X_tv = tsvd_xtv.fit_transform(X_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg = LogisticRegression()\n",
    "lg.fit(X_cv_tsvd, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87294776119402984"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(X_cv_tsvd, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "params_grid = {'penalty':['l1','l2'],\n",
    "              'C':[0.05, 0.5]}\n",
    "\n",
    "gs = GridSearchCV(LogisticRegression(),\n",
    "                  param_grid=params_grid,\n",
    "                 n_jobs=-1,\n",
    "                 verbose=2,\n",
    "                 cv=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 out of  20 | elapsed:   10.2s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   11.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [0.05, 0.5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_cv_tsvd, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.05, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "{'C': 0.05, 'penalty': 'l1'}\n",
      "0.823694029851\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_estimator_)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsb = gs.best_estimator_\n",
    "\n",
    "X_test_cv = cv.transform(X_test)\n",
    "X_test_svd = tsvd_cv.transform(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816666666667\n"
     ]
    }
   ],
   "source": [
    "print(gsb.score(X_test_svd, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict 0</th>\n",
       "      <th>Predict 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>2118</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>448</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predict 0  Predict 1\n",
       "Actual 0       2118         36\n",
       "Actual 1        448         38"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmat = confusion_matrix(y_test, gsb.predict(X_test_svd))\n",
    "df = pd.DataFrame(cmat, columns=['Predict 0', 'Predict 1'], index=['Actual 0', 'Actual 1'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.98      0.90      2154\n",
      "          1       0.51      0.08      0.14       486\n",
      "\n",
      "avg / total       0.77      0.82      0.76      2640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, gsb.predict(X_test_svd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Revisiting Pipelines\n",
    "\n",
    "We may also want to create a `Pipeline` object to reproducibly and reliably transform and predict data with one step versus many. We'll refactor our code here to take advantage of that and end with a check for understanding where you can practice the same. \n",
    "\n",
    "The first step is to break down our current model into a set of sequential steps. From my original example using the economic news dataset, my steps were:\n",
    "\n",
    "1. Apply `CountVectorizer` to `X` (matrix of article text)\n",
    "2. Apply `TruncatedSVD` with 10 components to the count vectorized data\n",
    "3. Predict using `RandomForestClassifier`\n",
    "\n",
    "These steps are _sequential_ and can feed one directly into the other. We do not need to include a feature union here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    CountVectorizer(stop_words='english'),\n",
    "    TruncatedSVD(n_components=10),\n",
    "    RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had more hyperparameters or different steps, we could change what is inside that pipeline to account for that. \n",
    "\n",
    "Next, let's fit this to our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll score it and run the predictions from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.982649253731\n",
      "[[4425    1]\n",
      " [  92  842]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      4426\n",
      "          1       1.00      0.90      0.95       934\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.score(X_train, y_train))\n",
    "predictions = pipeline.predict(X_train)\n",
    "print(confusion_matrix(y_train, predictions))\n",
    "print(classification_report(y_train, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we'll score and predict with our test set. We do not need to refit this to the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800757575758\n",
      "[[2077   77]\n",
      " [ 449   37]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.96      0.89      2154\n",
      "          1       0.32      0.08      0.12       486\n",
      "\n",
      "avg / total       0.73      0.80      0.75      2640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.score(X_test, y_test))\n",
    "predictions = pipeline.predict(X_test)\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check For Understanding 4 (15 Minutes)\n",
    "\n",
    "Pair up and do the following:\n",
    "\n",
    "1. Using the model you created in the last Check for Understanding, diagram out the steps involved in going from the training data to a fitted model with your partner. Highlight what steps can happen sequentially and which must happen in parallel (typically parallel steps would happen during the feature engineering stage)\n",
    "\n",
    "2. Pick a model and refactor the code to use a `Pipeline` instead of its current version. Write this code on the machine that doesn't have the original model on it. This is to help you think about what your code is *doing* versus how you've *written* it\n",
    "\n",
    "3. If you have time, replicate the process for the other model. \n",
    "\n",
    "If you used a grid search, feel free to set the best parameters manually (such as `RandomForestClassifier(n_estimators=100)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "    ...ty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('countvec', CountVectorizer(stop_words='english')), \n",
    "                 ('trunc', TruncatedSVD(n_components=1000)),\n",
    "                    ('log', LogisticRegression(penalty='l1', C=0.05))])\n",
    "\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82667910447761195"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81515151515151518"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2118   36]\n",
      " [ 452   34]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.98      0.90      2154\n",
      "          1       0.49      0.07      0.12       486\n",
      "\n",
      "avg / total       0.76      0.82      0.75      2640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, pipe.predict(X_test)))\n",
    "print(classification_report(y_test, pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gs = GridSearchCV(pipe, grid, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
