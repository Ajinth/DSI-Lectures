{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Neural Networks?\n",
    "\n",
    "Neural networks, in a single line, attempt to iteratively train a set (or sets) of weights that, when used together, return the most accurate predictions for a set of inputs. Just like many of our past models, the model is trained using a loss function, which our model will attempt to minimize over iterations. Remember that a loss function is some function that takes in our predictions and the actual values and returns some sort of aggregate value that shows how accurate (or not) we were.\n",
    "\n",
    "Neural networks do this by establishing sets of neurons (known as hidden layers) that take in some sort of input(s), apply a weight, and pass that output onward. As we feed more data into the network, it adjusts those weights based on the output of the loss function, until we have highly trained and specific weights.\n",
    "\n",
    "Why does one neuron turn out one way and a second neuron another? That's not generally something we can understand (though attempts have been made, such as Google's [Deep Dream](https://deepdreamgenerator.com/)). You can understand this as a kind of (very advanced) mathematic optimization.\n",
    "\n",
    "![](./images/neuralnet.png)\n",
    "\n",
    "Today, we're going to discuss some of the theory behind Neural Networks, particularly around the topology (the shape) of the network and how we iteratively fit the weights in the network to the data we've seen. We'll also write and investigate a (very small) neural network written using Python and Numpy. The next few days will take us through different types of networks and how we can create them using popular libraries like Keras and Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping out a Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of moving parts with Neural Networks. We'll be playing along with the very well known [Tensorflow Playground](http://playground.tensorflow.org/) as we talk about the structure of this neural networks. This website simulates a fairly small network in your browser and lets you tweak values and immediately realize their effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we know already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features\n",
    "\n",
    "Much like our other machine learning techniques, we do need to feed data into the network. For these smaller examples, we won't worry too much about the shape of our data, but typically having data normalized to be on scale between 0 and 1 (or -1 and 1) can help the network find a solution faster than when it would otherwise.\n",
    "\n",
    "#### Outputs\n",
    "\n",
    "Much like other supervised techniques, we need an output at the end as well. In most cases:\n",
    "\n",
    "- for a regression style technique, one output is usually fine\n",
    "- for a classification technique, one output per class is a good idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Concepts\n",
    "\n",
    "#### Neurons\n",
    "\n",
    "A neural network (at its core) is built up of different neurons that are linked together. Each takes in either the original input features or some transformed version of them and puts out a value (or set of values). One neuron looks something akin to this:\n",
    "\n",
    "![](./images/perceptron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neuron is going to be the combination of the following:\n",
    "\n",
    "- A **bias** term (akin to a constant or $B_0$ term in a linear regression)\n",
    "- The input terms they've received, each multiplied by a **weight**\n",
    "\n",
    "If our model has one neuron, this looks suspiciously similar to a linear regression:\n",
    "\n",
    "1. take each term\n",
    "2. multiply it by a weight\n",
    "3. sum those new values together \n",
    "4. add an additional bias term\n",
    "\n",
    "That output should, as we train our neural network, get closer and closer to what the output is for that specific set of inputs ($x_1...x_n$). As we'll see, the way we train the network and the way we transform our outputs (plus the number of neurons) distinguishes neural networks from linear regression quite strongly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layers\n",
    "\n",
    "What makes neural networks tick is the idea of hidden layers. Hidden does not mean anything particularly devious here, just that it is not the input or the output layer.\n",
    "\n",
    "Hidden layers can have:\n",
    "- any number of neurons per layer \n",
    "- can be of any number in your model**\n",
    "\n",
    "At each layer each neuron in that layer receives the same weight. However, each neuron is going to transform the data in a different way, based on how we assign or change the weights and bias in that neuron. \n",
    "\n",
    "![](./images/neuralnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the network above, we have two hidden layers and one output layer.\n",
    "\n",
    "- Hidden Layer 1\n",
    "    - 4 Neurons\n",
    "    - Each Neuron has 6 weights and 1 bias term\n",
    "    - Inputs: the original data\n",
    "    - Outputs: one number each\n",
    "- Hidden Layer 2\n",
    "    - 3 Neurons \n",
    "    - Each Neuron has 4 weights and 1 bias term\n",
    "    - Inputs: the four outputs from each Neuron in Hidden Layer 1\n",
    "    - Outputs: one number each\n",
    "- Output Layer\n",
    "    - 1 Neuron\n",
    "    - The one nNuron has 3 weights and 1 bias term\n",
    "    - Inputs: the three outputs from each Neuron in Hidden Layer 2\n",
    "    - Outputs: the final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 1 (3 Minutes)\n",
    "\n",
    "With a partner, answer the following about the network below:\n",
    "\n",
    "![](./images/percept.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. How many layers are in this network? Are they an input, output, or hidden layer? How many neurons are in each layer?\n",
    "2. For your hidden and output layers:\n",
    "    1. How many inputs does each neuron have?\n",
    "    2. How many outputs does each neuron have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation\n",
    "\n",
    "Neurons also have an activation function that transforms the output in a certain way. Some common examples are:\n",
    "\n",
    "- [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks): Also known as a Rectified Linear Unit, this turns the output to 0 if the output would be less than 0 (i.e., take the output and feed it through $f(y) = max(0, y)$). This means that the neuron is activated when its output is positive and not activated otherwise. This has the intuitive effect of turning a neuron \"on\" in certain cases and off in other cases.\n",
    "- [Softmax](https://en.wikipedia.org/wiki/Softmax_function): Used frequently at the output layer, this essentially \"squishes\" a bunch of inputs into a normalized scale of 0-1, which is great for creating something akin to a probability of falling into a given class. \n",
    "- [Sigmoid or Logistic](https://en.wikipedia.org/wiki/Logistic_function): Much like how we transformed the linear regression model to change the output to a zero or one through the use of a logistic or sigmoid function, we can do the same as an activation to squash the output to a scale between 0 and 1. \n",
    "\n",
    "There's a wealth of information on different types of activation functions within [this article](https://en.wikipedia.org/wiki/Activation_function) -- different activation functions, hidden layers, and neurons per layer can change how effective your neural network will be!\n",
    "\n",
    "Typically, we'll apply the weights in our neurons, add the bias and sum all terms, then pass that one value through the activation function, like this:\n",
    "\n",
    "![](./images/activation.png)\n",
    "\n",
    "### Picking a Topology for your Network\n",
    "\n",
    "There's no hard and fast rule for how to pick a topology for your neural network. Much like hyperparameters in other machine learning models, we're going to use a combination of experience, research, and exploration to come up with a topology that best suits our unique problem. One good place to start out is to (if you have a smaller set of input features) is a network with:\n",
    "\n",
    "1. The input layer\n",
    "2. One hidden layer with a number of neurons equal to the number of inputs\n",
    "3. One output layer with the appropriate activation function (softmax if you have a classification problem, no activation (or what's known as the identity function ($f(x) = x$) if you have a regression problem\n",
    "\n",
    "### Single Layer and Multilayer Perceptrons (SLP and MLP)\n",
    "\n",
    "The types of neural networks that we have discussed so far are known as perceptrons. A single layer perceptron **has no hidden layers** and is just a function of the inputs, weights, a bias term, and an activation function:\n",
    "\n",
    "![](./images/slp.png)\n",
    "\n",
    "Multilayer perceptrons (MLP) have 1 or more hidden layers in addition to their input and output layers. While SLPs are easiest to consider in an abstract sense, MLPs tend to be much more accurate and useful in practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 2 (5 Minutes plus Over Break)\n",
    "\n",
    "Independently or with a partner (your choice) -- pick two of the following activation functions:\n",
    "\n",
    "- Binary Step\n",
    "    - If $x \\le 0$: $f(x) = 0$; else $f(x) = 1$ \n",
    "- ReLU\n",
    "    - If $x \\le 0$: $f(x) = 0$; else $f(x) = x$\n",
    "- Logistic / Sigmoid\n",
    "    - $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- TanH\n",
    "    - $f(x) = \\tanh(x) = \\frac{2}{1 + e^{-2x}} - 1$\n",
    "- Softsign\n",
    "    - $f(x)={\\frac {x}{1+|x|}}$\n",
    "    \n",
    "Write a function in Python for each. Your functions should take in one value ($x$) and output the transformed version of $x$. \n",
    "\n",
    "> Note: $e$ can be found in the math library under `math.e` -- don't input this manually!\n",
    "\n",
    "We'll share our results in a thread on slack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/ (1 + math.e  ** (-1 * x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9241418199787566"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25d58aaae10>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25d5893a128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = list(range(-25, 26))\n",
    "ys = [sigmoid(x) for x in xs]\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training your network\n",
    "\n",
    "So far, we haven't discussed how a neural network assigns the \"right\" weights to each of its neurons. You could imagine a neural network with very bad weights that predicted poorly or, in some cases, completely counter to what it should. How do we train our network to have the right weights?\n",
    "\n",
    "Training our network to have the right weights relies on a couple of topics:\n",
    "\n",
    "1. A loss function that we are trying to optimize the values for\n",
    "2. The concept of forward and backward propogation \n",
    "3. Gradient Descent and a Learning Rate to make iterative changes to the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "We have encountered [loss functions](https://en.wikipedia.org/wiki/Loss_function) many times before -- they are functions that take in the actual values for the output and what our model predicts those values to be and gives us a value that we are trying to minimize with our model. Examples include:\n",
    "\n",
    "##### Regression: Sum of Squared Errors\n",
    "\n",
    "$$\\sum(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "##### Classification: Mean accuracy \n",
    "\n",
    "$$\\frac{n_{\\text{classified correctly}}}{n_{\\text{total}}}$$\n",
    "\n",
    "We're going to use loss functions in this context to tell us how well or how poorly our Neural Network is doing at that point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Forward and Backpropagation \n",
    "\n",
    "When we train our neural network, **we send our data multiple times through the network and tweak the weights each time**. This is an iterative process using forward and backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propagation\n",
    "\n",
    "Forward Propagation is straightforward -- either in batches or as individual observations, pass the training data through the network, applying all the weights, biases, and activation functions as usual. At this point, you should have actual and predicted values.\n",
    "\n",
    "#### Backpropagation\n",
    "\n",
    "What we want to do here is:\n",
    "\n",
    "1. See how far off we were from the truth using the loss function\n",
    "2. Identify which weights in our network are most responsible for how far we are off\n",
    "3. Change all of the weights to make our model more accurate, changing the weights that are \"the worst\" the most\n",
    "\n",
    "This is known as **Backpropagation** -- we are taking the errors we see in our model (as it stands currently) and are distributing them backwards to the rest of the layers. \n",
    "\n",
    "What we'll do is train our data in a number of full passes known as **epochs**. As modelers, we'll choose a number of epochs to train our model, essentially choosing a value to stop where we see no additional change in the accuracy of our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Weights (the Curse of Dimensionality) \n",
    "\n",
    "One question that you may have is \"Why don't we just try all of the weights and decisively find the best one?\" The problem here (which is often known as the curse of dimensionality) is that we have _so many different weights_ to try out (and so many different sets of values to exhaustively attempt) that it's impractical to try all of them. More than being impractical, for larger models with many weights and many possible values, we may still be estimating the \"best\" weights long past the [heat death of the universe](https://en.wikipedia.org/wiki/Heat_death_of_the_universe). \n",
    "\n",
    "#### **Estimating** the Weights \n",
    "\n",
    "What we'll do instead is use gradient descent to help us figure out what direction to tweak our weights. We've discussed gradient descent before, but as a quick primer: \n",
    "\n",
    "> Gradient Descent is an optimization technique to help us find the lowest point (global minima) of a function. The derivative (the rate of change in the function for a small change in the inputs) of a function at a given point tells which direction we would like to change our weights. We'll make changes of a certain size in the right direction until we hit a place where the derivative is zero (i.e., either direction will increase the output of the function) and will consider that our \"best\" place. \n",
    "\n",
    "- **Learning Rate**: The size of the change we make on each pass -- bigger learning rates mean we'll move faster, but may skip over places where there is actually a global minima\n",
    "- **Epoch**: The number of times we pass our data through the fitting process\n",
    "\n",
    "![](./images/gradient.png)\n",
    "\n",
    "For our purposes, the function we're trying to optimize is the loss function from above -- we want the value of that function to be as low as possible. Our loss function is a function of all of the weights in our network. What we'll do in an abstract sense is:\n",
    "\n",
    "1. Take the partial derivative (in other words, changing one weight and not all) of our loss function with respect to each weight in our model\n",
    "2. Based on that derivative, change the weights up or down\n",
    "\n",
    "The actual process is somewhat more complicated than that (see below), but the takeaway is backpropagation **looks at how badly we did on each pass, moves those errors back up the model, and then uses gradient descent to change the weight over a series of iterations**.\n",
    "\n",
    "![](./images/backprop.jpg)\n",
    "\n",
    "##### The Vanishing Gradient Problem\n",
    "\n",
    "One thing to keep in mind (and a good reason to not create networks that are very, very deep) is a phenomenon known as the Vanishing Gradient Problem. For networks with lots of hidden layers, earlier layers train at a much slower rate than layers closer to the output (the gradient \"vanishes\" as it gets deeper into the network, so the weights cannot change as much). That means that it can take a much longer time to train a deeper network without much increase in accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tackling Overfitting\n",
    "\n",
    "Do neural networks overfit? Yes, very much so. There are many weights to optimize and we can very quickly reach a point where the weights in each neuron are overfit to our training data (and therefore are limited in how well they predict new data).\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "Just like with linear models earlier in this course, we can also do regularization to make sure our weights are more generalizable. Because we're using our loss function to determine how we should change our weights, if we penalize the loss function to avoid larger weights, we will see the same behavior as we did with linear models -- weights will be large or impactful only if they contribute sufficiently to how well the model fits as a whole. \n",
    "\n",
    "#### Dropout\n",
    "\n",
    "We can also use something known as dropout to tackle overfitting. Dropout will turn off a random percentage of neurons in each pass (user-defined). This prevents each layer from fitting too strongly to a given input and therefore wards off overfitting. \n",
    "\n",
    "![](./images/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 3 (10 Minutes)\n",
    "\n",
    "In pairs, use the [Tensorflow Playground](http://playground.tensorflow.org/) with the following settings:\n",
    "\n",
    "![](./images/settings.png)\n",
    "\n",
    "> make sure that you're using a classification problem (upper right corner as well!)\n",
    "\n",
    "1. In your own words, define the following and forecast what changing each of them will do to your model?\n",
    "    1. Learning Rate\n",
    "    2. Activation Function\n",
    "    3. Regularization and Regularization Rate\n",
    "2. Work together with the settings established above to create a (mini!) neural network that best predicts test data. Feel free to try any (or all!) of the inputs, any number of neurons, hidden layers, learning rates, regularization parameters, and activation functions. A good place to start is the following:\n",
    "\n",
    "![](./images/starting.png)\n",
    "\n",
    "We'll share our results in Slack after 10 minutes. Include your test loss on the thread. We'll ask the pair with the best score to share their settings for the best possible network. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
